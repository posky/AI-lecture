{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glob\n",
    "* glob.glob()\n",
    "    * 디렉토리, 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./paddy-disease-classification/train_images\\\\bacterial_leaf_blight\\\\', './paddy-disease-classification/train_images\\\\bacterial_leaf_streak\\\\', './paddy-disease-classification/train_images\\\\bacterial_panicle_blight\\\\', './paddy-disease-classification/train_images\\\\blast\\\\', './paddy-disease-classification/train_images\\\\brown_spot\\\\', './paddy-disease-classification/train_images\\\\dead_heart\\\\', './paddy-disease-classification/train_images\\\\downy_mildew\\\\', './paddy-disease-classification/train_images\\\\hispa\\\\', './paddy-disease-classification/train_images\\\\normal\\\\', './paddy-disease-classification/train_images\\\\tungro\\\\']\n",
      "\n",
      "479 \t bacterial_leaf_blight\n",
      "380 \t bacterial_leaf_streak\n",
      "337 \t bacterial_panicle_blight\n",
      "1738 \t blast\n",
      "965 \t brown_spot\n",
      "1442 \t dead_heart\n",
      "620 \t downy_mildew\n",
      "1594 \t hispa\n",
      "1764 \t normal\n",
      "1088 \t tungro\n",
      "3469 \t test_images\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "train_path = './paddy-disease-classification/train_images/'\n",
    "test_path = './paddy-disease-classification/test_images/'\n",
    "\n",
    "print(glob.glob(train_path + '/*/'))\n",
    "print()\n",
    "\n",
    "for filepath in glob.glob(train_path + '/*/'):\n",
    "    files = glob.glob(filepath + '*')\n",
    "    print(f'{len(files)} \\t {Path(filepath).name}')\n",
    "\n",
    "files = glob.glob(test_path + '*')\n",
    "print(f'{len(files)} \\t {Path(test_path).name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from os import listdir\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from psutil import virtual_memory\n",
    "\n",
    "\n",
    "SEED = 123\n",
    "EPOCHS = 25\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "DEPTH = 3\n",
    "\n",
    "n_classes = len(glob.glob(train_path + '/*/'))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "* 뉴런 활성화 여부\n",
    "* 과적합 방지 목적\n",
    "\n",
    "### BatchNormalization\n",
    "* 층으로 들어가는 입력값이 한쪽으로 쏠리거나 너무 퍼지거나 너무 좁이지지 않게 해주는 기법\n",
    "* 각 층의 입력값의 평균과 표준편차 다시 맞춤\n",
    "\n",
    "### optimizer\n",
    "* learning_rate\n",
    "* decay - learning_rate 감소 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 256, 256, 32)      0         \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 256, 256, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 85, 85, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 85, 85, 32)        0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 85, 85, 64)        18496     \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 85, 85, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 85, 85, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 85, 85, 64)        36928     \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 85, 85, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 85, 85, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 42, 42, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 42, 42, 64)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 42, 42, 128)       73856     \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 42, 42, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 42, 42, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 42, 42, 128)       147584    \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 42, 42, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 42, 42, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 21, 21, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 21, 21, 128)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 56448)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              57803776  \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,097,546\n",
      "Trainable params: 58,094,666\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (HEIGHT, WIDTH, DEPTH)\n",
    "    chan_dim = -1\n",
    "    print(K.image_data_format())\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (DEPTH, HEIGHT, WIDTH)\n",
    "        chan_dim = 1\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=chan_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=chan_dim))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=chan_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=chan_dim))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=chan_dim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    # distribution\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator\n",
    "* rescale: 이미지 처리 마지막에 주어진 값 곱해 크기 조정.\n",
    "* validation_split: train, valid 이미지 자동으로 나누기. subset 통해 부름.\n",
    "* rotation_rage: 사진 회전시킬 각도 (0~180)\n",
    "* shear_range: shearing transformation (0.3 -> 30%). 이미지 찌그러뜨림.\n",
    "* zoom_range: 사진 확대 또는 축소 범위 (0.3 -> 30%)\n",
    "* horizontal_flip: 랜덤하게 이미지 수평으로 뒤집기\n",
    "* width_shift_range: 사진을 수평으로 평행이동 시킬 범위 (0 이하 전체의 비율, 0이상 픽셀 단위)\n",
    "* height_shift_range: 무작위로 이미지를 좌우로 뒤집기. 수평 대칭을 가정할 수 있을 때 사용. (풍경/인물 사진)\n",
    "* vertical_flip: 무작위로 이미지를 상하로 뒤집기. 수직 대칭을 가정할 수 있을 때 사용.\n",
    "\n",
    "* featurewise_center: 데이터셋에 대해 특성별로 인풋의 평균이 0이 되도록\n",
    "* samplewise_center: 각 샘플의 평균이 0이 되도록\n",
    "* featurewise_std_normalization: 인풋을 각 특성 내에서 데이터셋의 표준편차로 나눔\n",
    "* samplewise_std_normalization: 각 인풋을 표준편차로 나상상\n",
    "* zca_whitening: 영위상 성분분석 백색화\n",
    "* channel_shift_range: 무작위 채널 이동 범위\n",
    "* fill_mode: 회전 축소 등으로 이미지에 여백이 생겼을때 채우는 방법\n",
    "    * nearest: 가장 가까운 픽셀 찾아 늘림 (default)\n",
    "    * constant: 다음에 있는 cval값으로 채움\n",
    "    * reflect: 원본 이미지 대칭\n",
    "    * wrap: 원본 이미지 이어 붙임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8330 images belonging to 10 classes.\n",
      "Found 2077 images belonging to 10 classes.\n",
      "Found 3469 images belonging to 1 classes.\n",
      "{'bacterial_leaf_blight': 0, 'bacterial_leaf_streak': 1, 'bacterial_panicle_blight': 2, 'blast': 3, 'brown_spot': 4, 'dead_heart': 5, 'downy_mildew': 6, 'hispa': 7, 'normal': 8, 'tungro': 9}\n",
      "3469\n"
     ]
    }
   ],
   "source": [
    "image_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = image_datagen.flow_from_directory(\n",
    "    directory = train_path,\n",
    "    subset='training',\n",
    "    target_size=(WIDTH, HEIGHT),\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "valid_generator = image_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    subset='validation',\n",
    "    target_size=(WIDTH, HEIGHT),\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(WIDTH, HEIGHT),\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    "    classes=['.'],\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "print(test_generator.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "260/260 [==============================] - 224s 839ms/step - loss: 0.3749 - accuracy: 0.3426 - val_loss: 1.5806 - val_accuracy: 0.0747\n",
      "Epoch 2/25\n",
      " 50/260 [====>.........................] - ETA: 54s - loss: 0.2750 - accuracy: 0.4119"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\engine\\training.py:2260\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2249\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2250\u001b[0m \n\u001b[0;32m   2251\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[39m  this endpoint.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2256\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2257\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2258\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2259\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m-> 2260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2261\u001b[0m     generator,\n\u001b[0;32m   2262\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2263\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2264\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2265\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2266\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2267\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2268\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2269\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2273\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2274\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\posky\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n // valid_generator.batch_size\n",
    "history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#Train and validation loss\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "test_generator.reset()\n",
    "pred = model.predict_generator(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(\n",
    "    test_generator,\n",
    "    steps=STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    ")\n",
    "print(acc, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_generator.classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(test_generator.classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices = np.argmax(pred, axis=1)\n",
    "labels = (train_generator.class_indices)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v, k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"image_id\": filenames,\n",
    "    \"label\": predictions\n",
    "})\n",
    "results.image_id = results.image_id.str.replace('./', '')\n",
    "results.to_csv(\"submission.csv\", index=False)\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf20')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6031b87587c83867ea9d6a230f58cff6e73fb05ff5d2fb7f6404d0bbff37737a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
