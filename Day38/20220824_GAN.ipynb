{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    print('data range:', np.min(X_train), np.max(X_train))\n",
    "    # convert shape of X_train from (60000, 28, 28) to (60000, 784)\n",
    "    # 784 columns per row\n",
    "    X_train = X_train.reshape(60000, 784)\n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = load_data()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(units=256, input_dim=100))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(units=512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(units=1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(units=784, activation='tanh'))\n",
    "\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return generator\n",
    "\n",
    "g = create_generator()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(units=1024, input_dim=784))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(units=512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(units=256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return discriminator\n",
    "\n",
    "d = create_discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(discriminator, generator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(100,))\n",
    "    X = generator(gan_input)\n",
    "    gan_output = discriminator(X)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)  # 레이어를 객체로 그룹화\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan\n",
    "\n",
    "gan = create_gan(d, g)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(loc=0, scale=1, size=[examples, 100])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(100, 28, 28)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'gan_generated_image {epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs=1, batch_size=128):\n",
    "    # Loading the data\n",
    "    (X_train, y_train, X_test, y_test) = load_data()\n",
    "    batch_count = X_train.shape[0] / batch_size\n",
    "\n",
    "    # Creating GAN\n",
    "    generator = create_generator()\n",
    "    discriminator = create_discriminator()\n",
    "    gan = create_gan(discriminator, generator)\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        print(f'Epoch {e}')\n",
    "        for _ in tqdm(range(batch_size)):\n",
    "            # generate random noise as an input to initialize the generator\n",
    "            noise = np.random.normal(0, 1, [batch_size, 100])\n",
    "\n",
    "            # Generate fake MNIST images from noised input\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Get a random set of real images\n",
    "            image_batch = X_train[np.random.randint(low=0, high=X_train.shape[0], size=batch_size)]\n",
    "\n",
    "            # Construct different batches of real and fake data\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "\n",
    "            # Pre train discriminator on fake and real data before starting the gan.\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Tricking the noised input of the Generator as real data\n",
    "            noise = np.random.normal(0, 1, [batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "\n",
    "            # During the trainig of gan, the weights of discriminator\n",
    "            # shoud be fixed. We can enforce that by setting the trainable flag\n",
    "            discriminator.trainable = False\n",
    "\n",
    "            # training the GAN by alternating the training of the Discriminator\n",
    "            # and training the chained GAN model with Discriminator's weights freezed.\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# epoch: 200, batch_size: 256\n",
    "training(200, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf20')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6031b87587c83867ea9d6a230f58cff6e73fb05ff5d2fb7f6404d0bbff37737a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
